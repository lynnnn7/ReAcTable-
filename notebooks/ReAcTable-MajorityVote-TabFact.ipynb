{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7fa0fb0-dbd8-4ec5-8b5d-b9c237455edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54b74f4-099c-4f47-89f2-305d54d6c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from tabqa.GptPrompter import *\n",
    "from tabqa.GptCOTPrompter import *\n",
    "from tabqa.GptCOTPrompter_BeamSeach import *\n",
    "from tabqa.AutoReasoner import *\n",
    "import dotenv\n",
    "\n",
    "config = dotenv.dotenv_values(\"../.env\")\n",
    "openai.api_key = config['OPENAI_API_KEY']\n",
    "openai.base_url = config['GPT_BASE_URL']\n",
    "\n",
    "dataset = pd.read_csv('../dataset/WikiTableQuestions/data/pristine-unseen-tables.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ad51ccf-cda6-47ed-8fb9-8f7f60343867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 1455.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given\n",
      "Error: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given\n",
      "Error: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given\n",
      "Error: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given\n",
      "Error: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given\n",
      "Error: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given\n",
      "Error: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given\n",
      "Error: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given\n",
      "Error: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given\n",
      "Error: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given\n",
      "Error: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given\n",
      "Error: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given\n",
      "Error: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given\n",
      "Error: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given\n",
      "Error: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given\n"
     ]
    }
   ],
   "source": [
    "NNDemo = False\n",
    "max_demo = 5\n",
    "# gpt_model = 'gpt-4o-mini'\n",
    "gpt_model = 'Qwen/Qwen2.5-7B-Instruct'\n",
    "\n",
    "program = 'sql-py'\n",
    "template = 'original-sql-py-no-intermediate'\n",
    "\n",
    "def parallel_codex_func_formatv1(i):\n",
    "    max_retry = 3\n",
    "    while max_retry>0:\n",
    "        try:\n",
    "            codex_prompter = CodexAnswerCOTExecutor_HighTemperaturMajorityVote(\n",
    "                                                f'prompt_template/{template}.json',\n",
    "                                                dataset.iloc[i]['id'], \n",
    "                                                dataset.iloc[i]['utterance'], \n",
    "                                                dataset.iloc[i]['context'], \n",
    "                                                dataset.iloc[i]['targetValue'],  \n",
    "                                                base_path='../dataset/WikiTableQuestions/',\n",
    "                                                demo_file=f'few-shot-demo/WikiTQ-{program}.json',\n",
    "                                                )\n",
    "            codex_prompter.max_demo = max_demo\n",
    "            codex_prompter.model = gpt_model\n",
    "            codex_prompter._gen_gpt_prompt(NNDemo)\n",
    "            codex_prompter._get_gpt_prediction_majority_vote(repeat_times=5)\n",
    "            log = codex_prompter._log_dict()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            log = {\n",
    "                'id': dataset.iloc[i]['id'],\n",
    "                'uncaught_err': str(e)\n",
    "            }\n",
    "            if \"model's maximum context length\" in str(e):\n",
    "                return log\n",
    "            max_retry -= 1\n",
    "    return log\n",
    "    \n",
    "n_threads = 3\n",
    "maxLimit = 5\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "output_result_file = f'../dataset/WikiTableQuestions/results/CodexAnswerCOTExecutor_HighTemperaturMajorityVote_{template}_{program}_NNDemo={NNDemo}_results_pristine-unseen-tables_limit{maxLimit}_model{gpt_model}.json'\n",
    "output_dir = os.path.dirname(output_result_file)  # 获取文件的目录部分\n",
    "if not os.path.exists(output_dir):  # 如果目录不存在\n",
    "    os.makedirs(output_dir)  # 创建目录\n",
    "logs = Parallel(n_jobs=n_threads, require='sharedmem')(delayed(parallel_codex_func_formatv1)(i) for i in tqdm(range(min(maxLimit, dataset.shape[0]))))    \n",
    "json.dump(logs, open(output_result_file, 'w'), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2a05ddc-a700-4062-85ac-7f70ecd46e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading dataset from ./tagged/data/pristine-seen-tables.tagged\n",
      "Reading dataset from ./tagged/data/pristine-unseen-tables.tagged\n",
      "Reading dataset from ./tagged/data/training.tagged\n",
      "Read 22033 examples\n",
      "Reading predictions from ./results/CodexAnswerCOTExecutor_HighTemperaturMajorityVote_original-sql-py-no-intermediate_sql-py_NNDemo=False_results_pristine-unseen-tables_limit5_modelgpt-4o-mini.json\n",
      "Examples: 5\n",
      "Correct: 2\n",
      "Accuracy: 0.4\n",
      "GPT error: 0.0\n",
      "Uncaught error: 0.0\n",
      "{}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate: \n",
    "os.system(f'cd ../dataset/WikiTableQuestions/ && python2 evaluator.py ./results/{output_result_file.split(\"/\")[-1]} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5ba55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reactable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
