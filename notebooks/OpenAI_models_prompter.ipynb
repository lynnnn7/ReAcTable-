{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da319532-a047-4090-bc7e-12b259c31fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071088c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "# from gpt3_sandbox.api.gpt import GPT\n",
    "# from gpt3_sandbox.api.gpt import Example\n",
    "from pandasql import sqldf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "openai.api_key = API_key\n",
    "# modellist = openai.Model.list()\n",
    "# print('All models:')\n",
    "# for i in range(len(openai.Model.list()['data'])):\n",
    "#     print(modellist['data'][i]['id'])\n",
    "from GptPrompter import *\n",
    "from AutoReasoner import *\n",
    "# ft = fasttext.load_model('/mnt/idm_automapping/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96979b8-4829-4ad7-bb8b-96e6a846e42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b65d5737",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4344, 4)\n",
      "id                                                          nu-0\n",
      "utterance      which country had the most cyclists finish wit...\n",
      "context                                      csv/203-csv/733.csv\n",
      "targetValue                                                Italy\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4344/4344 [06:38<00:00, 10.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nu-138 The server is overloaded or not ready yet.\n",
      "nu-514 This model's maximum context length is 8193 tokens, however you requested 9774 tokens (9646 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-573 This model's maximum context length is 8193 tokens, however you requested 13195 tokens (13067 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-1365 The server is overloaded or not ready yet.\n",
      "nu-3092 This model's maximum context length is 8193 tokens, however you requested 9018 tokens (8890 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3336 The server is overloaded or not ready yet.\n",
      "nu-147 The server is overloaded or not ready yet.\n",
      "nu-2047 This model's maximum context length is 8193 tokens, however you requested 11525 tokens (11397 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-2243 This model's maximum context length is 8193 tokens, however you requested 11528 tokens (11400 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-2900 This model's maximum context length is 8193 tokens, however you requested 13662 tokens (13534 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3228 This model's maximum context length is 8193 tokens, however you requested 10911 tokens (10783 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3290 This model's maximum context length is 8193 tokens, however you requested 8965 tokens (8837 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3711 This model's maximum context length is 8193 tokens, however you requested 10908 tokens (10780 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-4085 This model's maximum context length is 8193 tokens, however you requested 9782 tokens (9654 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-124 The server is overloaded or not ready yet.\n",
      "nu-1343 The server is overloaded or not ready yet.\n",
      "nu-150 The server is overloaded or not ready yet.\n",
      "nu-1707 This model's maximum context length is 8193 tokens, however you requested 11731 tokens (11603 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-141 The server is overloaded or not ready yet.\n",
      "nu-208 This model's maximum context length is 8193 tokens, however you requested 13199 tokens (13071 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3327 The server is overloaded or not ready yet.\n",
      "nu-132 The server is overloaded or not ready yet.\n",
      "nu-642 This model's maximum context length is 8193 tokens, however you requested 9774 tokens (9646 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-718 This model's maximum context length is 8193 tokens, however you requested 9018 tokens (8890 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-2100 This model's maximum context length is 8193 tokens, however you requested 8963 tokens (8835 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3750 This model's maximum context length is 8193 tokens, however you requested 10910 tokens (10782 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-125 The server is overloaded or not ready yet.\n",
      "nu-279 This model's maximum context length is 8193 tokens, however you requested 10907 tokens (10779 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-2633 This model's maximum context length is 8193 tokens, however you requested 11523 tokens (11395 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-148 The server is overloaded or not ready yet.\n",
      "nu-659 This model's maximum context length is 8193 tokens, however you requested 13663 tokens (13535 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-1073 This model's maximum context length is 8193 tokens, however you requested 9775 tokens (9647 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-1895 This model's maximum context length is 8193 tokens, however you requested 13194 tokens (13066 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-2816 This model's maximum context length is 8193 tokens, however you requested 8984 tokens (8856 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3335 The server is overloaded or not ready yet.\n",
      "nu-30 This model's maximum context length is 8193 tokens, however you requested 8964 tokens (8836 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-158 This model's maximum context length is 8193 tokens, however you requested 9625 tokens (9497 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-690 This model's maximum context length is 8193 tokens, however you requested 13194 tokens (13066 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-936 This model's maximum context length is 8193 tokens, however you requested 10912 tokens (10784 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-999 This model's maximum context length is 8193 tokens, however you requested 11732 tokens (11604 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-1596 This model's maximum context length is 8193 tokens, however you requested 9015 tokens (8887 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-1873 This model's maximum context length is 8193 tokens, however you requested 13195 tokens (13067 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-1877 This model's maximum context length is 8193 tokens, however you requested 10909 tokens (10781 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3469 This model's maximum context length is 8193 tokens, however you requested 9783 tokens (9655 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3573 This model's maximum context length is 8193 tokens, however you requested 13660 tokens (13532 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-4188 This model's maximum context length is 8193 tokens, however you requested 9784 tokens (9656 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-4222 This model's maximum context length is 8193 tokens, however you requested 9021 tokens (8893 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-151 The server is overloaded or not ready yet.\n",
      "nu-575 This model's maximum context length is 8193 tokens, however you requested 11528 tokens (11400 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-904 This model's maximum context length is 8193 tokens, however you requested 13665 tokens (13537 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-919 This model's maximum context length is 8193 tokens, however you requested 8970 tokens (8842 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-1624 This model's maximum context length is 8193 tokens, however you requested 13200 tokens (13072 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3253 This model's maximum context length is 8193 tokens, however you requested 13663 tokens (13535 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3419 This model's maximum context length is 8193 tokens, however you requested 11734 tokens (11606 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3663 This model's maximum context length is 8193 tokens, however you requested 9779 tokens (9651 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-142 The server is overloaded or not ready yet.\n",
      "nu-2729 This model's maximum context length is 8193 tokens, however you requested 11733 tokens (11605 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-143 The server is overloaded or not ready yet.\n",
      "nu-1330 This model's maximum context length is 8193 tokens, however you requested 9020 tokens (8892 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-1955 This model's maximum context length is 8193 tokens, however you requested 9018 tokens (8890 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-2160 This model's maximum context length is 8193 tokens, however you requested 13193 tokens (13065 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-136 The server is overloaded or not ready yet.\n",
      "nu-2797 This model's maximum context length is 8193 tokens, however you requested 13193 tokens (13065 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-144 The server is overloaded or not ready yet.\n",
      "nu-1346 The server is overloaded or not ready yet.\n",
      "nu-137 The server is overloaded or not ready yet.\n",
      "nu-1185 This model's maximum context length is 8193 tokens, however you requested 8963 tokens (8835 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-2212 This model's maximum context length is 8193 tokens, however you requested 9023 tokens (8895 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-2292 This model's maximum context length is 8193 tokens, however you requested 10916 tokens (10788 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-2293 This model's maximum context length is 8193 tokens, however you requested 13194 tokens (13066 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-2650 This model's maximum context length is 8193 tokens, however you requested 11522 tokens (11394 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-139 The server is overloaded or not ready yet.\n",
      "nu-263 This model's maximum context length is 8193 tokens, however you requested 8970 tokens (8842 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-1406 This model's maximum context length is 8193 tokens, however you requested 8970 tokens (8842 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-2565 This model's maximum context length is 8193 tokens, however you requested 13200 tokens (13072 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-149 The server is overloaded or not ready yet.\n",
      "nu-401 This model's maximum context length is 8193 tokens, however you requested 13199 tokens (13071 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3487 This model's maximum context length is 8193 tokens, however you requested 9777 tokens (9649 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-140 The server is overloaded or not ready yet.\n",
      "nu-444 This model's maximum context length is 8193 tokens, however you requested 9776 tokens (9648 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-1004 This model's maximum context length is 8193 tokens, however you requested 13663 tokens (13535 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-1351 The server is overloaded or not ready yet.\n",
      "nu-2323 This model's maximum context length is 8193 tokens, however you requested 9020 tokens (8892 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-2673 This model's maximum context length is 8193 tokens, however you requested 9019 tokens (8891 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-2819 This model's maximum context length is 8193 tokens, however you requested 9775 tokens (9647 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-2906 This model's maximum context length is 8193 tokens, however you requested 9618 tokens (9490 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3294 This model's maximum context length is 8193 tokens, however you requested 11518 tokens (11390 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3662 This model's maximum context length is 8193 tokens, however you requested 9014 tokens (8886 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-146 The server is overloaded or not ready yet.\n",
      "nu-367 This model's maximum context length is 8193 tokens, however you requested 11525 tokens (11397 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-1140 This model's maximum context length is 8193 tokens, however you requested 13198 tokens (13070 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-1166 This model's maximum context length is 8193 tokens, however you requested 9013 tokens (8885 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-1627 This model's maximum context length is 8193 tokens, however you requested 13196 tokens (13068 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-2219 This model's maximum context length is 8193 tokens, however you requested 11729 tokens (11601 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3693 This model's maximum context length is 8193 tokens, however you requested 9019 tokens (8891 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-145 The server is overloaded or not ready yet.\n",
      "nu-1671 This model's maximum context length is 8193 tokens, however you requested 13664 tokens (13536 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-3139 This model's maximum context length is 8193 tokens, however you requested 13199 tokens (13071 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "nu-4068 This model's maximum context length is 8193 tokens, however you requested 13195 tokens (13067 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n"
     ]
    }
   ],
   "source": [
    "demo_num = 0\n",
    "at_index = None\n",
    "\n",
    "def parallel_codex_func(i):\n",
    "    try:\n",
    "        codex_prompter = CodexSQL(training_df.iloc[i]['id'], \n",
    "                                  training_df.iloc[i]['utterance'], \n",
    "                                  training_df.iloc[i]['context'], \n",
    "                                  training_df.iloc[i]['targetValue'], \n",
    "                                  base_path='./dataset/WikiTableQuestions/',\n",
    "                                  demo_file='./dataset/WikiTableQuestions/examples-sql.txt'\n",
    "                                 )\n",
    "        codex_prompter._gen_codex_prompt(demo_num=demo_num, at_index=at_index)\n",
    "        codex_prompter._get_gpt_prediction()\n",
    "        if codex_prompter.gpt_error is None:\n",
    "            codex_prompter._evaluate_result(verbose=False)\n",
    "        return codex_prompter._log_dict()\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'id': training_df.iloc[i]['id'],\n",
    "            'uncaught_err': str(e)\n",
    "        }\n",
    "\n",
    "def parallel_gpt_answer_func(i):\n",
    "    try:\n",
    "        prompter = GptAnswer(training_df.iloc[i]['id'], \n",
    "                             training_df.iloc[i]['utterance'], \n",
    "                             training_df.iloc[i]['context'], \n",
    "                             training_df.iloc[i]['targetValue'], \n",
    "                             base_path='./dataset/WikiTableQuestions/',\n",
    "                             demo_file='./dataset/WikiTableQuestions/examples.txt')\n",
    "        prompter._gen_gpt_prompt(demo_num=demo_num, at_index=at_index)\n",
    "        prompter._get_gpt_prediction()\n",
    "        if prompter.gpt_error is None:\n",
    "            prompter._evaluate_result(verbose=False)\n",
    "        return prompter._log_dict()\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'id': training_df.iloc[i]['id'],\n",
    "            'uncaught_err': str(e)\n",
    "        }\n",
    "\n",
    "def parallel_codex_answer_func(i):\n",
    "    try:\n",
    "        prompter = GptAnswer(training_df.iloc[i]['id'], \n",
    "                             training_df.iloc[i]['utterance'], \n",
    "                             training_df.iloc[i]['context'], \n",
    "                             training_df.iloc[i]['targetValue'], \n",
    "                             base_path='./dataset/WikiTableQuestions/',\n",
    "                             demo_file='./dataset/WikiTableQuestions/examples.txt')\n",
    "        prompter.model = \"davinci-codex-002-msft\"\n",
    "        prompter._gen_gpt_prompt(demo_num=demo_num, at_index=at_index)\n",
    "        prompter._get_gpt_prediction()\n",
    "        if prompter.gpt_error is None:\n",
    "            prompter._evaluate_result(verbose=False)\n",
    "        return prompter._log_dict()\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'id': training_df.iloc[i]['id'],\n",
    "            'uncaught_err': str(e)\n",
    "        }\n",
    "    \n",
    "def parallel_gpt_instruct_func(i):\n",
    "    try:\n",
    "        prompter = GptInstruct(\n",
    "            training_df.iloc[i]['id'], \n",
    "            training_df.iloc[i]['utterance'], \n",
    "            training_df.iloc[i]['context'], \n",
    "            training_df.iloc[i]['targetValue'], \n",
    "            base_path='./dataset/WikiTableQuestions/',\n",
    "        )\n",
    "        prompter._gen_gpt_prompt()\n",
    "        prompter._get_gpt_prediction()\n",
    "        if prompter.gpt_error is None:\n",
    "            prompter._evaluate_result(verbose=False)\n",
    "        return prompter._log_dict()\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'id': training_df.iloc[i]['id'],\n",
    "            'uncaught_err': str(e)\n",
    "        }\n",
    "\n",
    "def parallel_GptAnswerReason_func(i):\n",
    "    try:\n",
    "        prompter = GptAnswerReason(\n",
    "            training_df.iloc[i]['id'], \n",
    "            training_df.iloc[i]['utterance'], \n",
    "            training_df.iloc[i]['context'], \n",
    "            training_df.iloc[i]['targetValue'], \n",
    "            base_path='./dataset/WikiTableQuestions/',\n",
    "            demo_file='./dataset/WikiTableQuestions/reasoning-examples.txt'\n",
    "        )\n",
    "        prompter._gen_gpt_prompt(demo_num=demo_num)\n",
    "        prompter._get_gpt_prediction()\n",
    "        if prompter.gpt_error is None:\n",
    "            prompter._evaluate_result(verbose=False)\n",
    "        return prompter._log_dict()\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'id': training_df.iloc[i]['id'],\n",
    "            'uncaught_err': str(e)\n",
    "        }   \n",
    "\n",
    "def parallel_CodexAnswerReason_func(i):\n",
    "    try:\n",
    "        prompter = CodexAnswerReason(\n",
    "            training_df.iloc[i]['id'], \n",
    "            training_df.iloc[i]['utterance'], \n",
    "            training_df.iloc[i]['context'], \n",
    "            training_df.iloc[i]['targetValue'], \n",
    "            base_path='./dataset/WikiTableQuestions/',\n",
    "            demo_file='./dataset/WikiTableQuestions/reasoning-examples.txt'\n",
    "        )\n",
    "        prompter.model = \"davinci-codex-002-msft\"\n",
    "        prompter._gen_gpt_prompt()\n",
    "        prompter._get_gpt_prediction()\n",
    "        if prompter.gpt_error is None:\n",
    "            prompter._evaluate_result(verbose=False)\n",
    "        return prompter._log_dict()\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'id': training_df.iloc[i]['id'],\n",
    "            'uncaught_err': str(e)\n",
    "        }   \n",
    "    \n",
    "def parallel_CodexSQLReason_func(i):\n",
    "    try:\n",
    "        prompter = CodexSQLReason(\n",
    "            training_df.iloc[i]['id'], \n",
    "            training_df.iloc[i]['utterance'], \n",
    "            training_df.iloc[i]['context'], \n",
    "            training_df.iloc[i]['targetValue'], \n",
    "            base_path='./dataset/WikiTableQuestions/',\n",
    "            demo_file='./dataset/WikiTableQuestions/reasoning-examples-sql.txt'\n",
    "        )\n",
    "        prompter._gen_gpt_prompt(demo_num=demo_num)\n",
    "        prompter._get_gpt_prediction()\n",
    "        if prompter.gpt_error is None:\n",
    "            prompter._evaluate_result(verbose=False)\n",
    "        return prompter._log_dict()\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'id': training_df.iloc[i]['id'],\n",
    "            'uncaught_err': str(e)\n",
    "        }\n",
    "    \n",
    "maxLimit = np.inf \n",
    "# maxLimit = 20\n",
    "splits = ['pristine-unseen-tables']\n",
    "\n",
    "# for split in splits:\n",
    "#     training_df = pd.read_csv(f'./dataset/WikiTableQuestions/data/{split}.tsv', sep='\\t', on_bad_lines='warn')\n",
    "#     print(training_df.shape)\n",
    "#     print(training_df.iloc[0])\n",
    "#     n_threads = 40\n",
    "#     from joblib import Parallel, delayed\n",
    "#     logs = Parallel(n_jobs=n_threads)(delayed(parallel_codex_func)(i) for i in tqdm(range(min(maxLimit, training_df.shape[0]))))\n",
    "#     # logs = Parallel(n_jobs=n_threads)(delayed(parallel_codex_func)(i) for i in tqdm(range(5)))\n",
    "#     json.dump(logs, open(f'./dataset/WikiTableQuestions/results/CodexSQL_fewShot_demoNum{demo_num}_atIndex{at_index}_results_{split}.json', 'w'))\n",
    "\n",
    "# for split in splits:\n",
    "#     training_df = pd.read_csv(f'./dataset/WikiTableQuestions/data/{split}.tsv', sep='\\t', on_bad_lines='warn')\n",
    "#     print(training_df.shape)\n",
    "#     print(training_df.iloc[0])\n",
    "#     n_threads = 40\n",
    "#     from joblib import Parallel, delayed\n",
    "#     logs = Parallel(n_jobs=n_threads)(delayed(parallel_gpt_answer_func)(i) for i in tqdm(range(min(maxLimit, training_df.shape[0]))))\n",
    "#     json.dump(logs, open(f'./dataset/WikiTableQuestions/results/GptAnswer_fewShot_demoNum{demo_num}_atIndex{at_index}_results_{split}.json', 'w'))\n",
    "\n",
    "# for split in splits:\n",
    "#     training_df = pd.read_csv(f'./dataset/WikiTableQuestions/data/{split}.tsv', sep='\\t', on_bad_lines='warn')\n",
    "#     print(training_df.shape)\n",
    "#     print(training_df.iloc[0])\n",
    "#     n_threads = 40\n",
    "#     from joblib import Parallel, delayed\n",
    "#     logs = Parallel(n_jobs=n_threads)(delayed(parallel_codex_answer_func)(i) for i in tqdm(range(min(maxLimit, training_df.shape[0]))))\n",
    "#     # logs = Parallel(n_jobs=n_threads)(delayed(parallel_gpt_answer_func)(i) for i in tqdm(range(5)))\n",
    "    # json.dump(logs, open(f'./dataset/WikiTableQuestions/results/CodexAnswer_fewShot_demoNum{demo_num}_atIndex{at_index}_results_{split}.json', 'w'))\n",
    "    \n",
    "# for split in splits:\n",
    "#     training_df = pd.read_csv(f'./dataset/WikiTableQuestions/data/{split}.tsv', sep='\\t', on_bad_lines='warn')\n",
    "#     print(training_df.shape)\n",
    "#     print(training_df.iloc[0])\n",
    "#     n_threads = 20\n",
    "#     from joblib import Parallel, delayed\n",
    "#     logs = Parallel(n_jobs=n_threads)(delayed(parallel_gpt_instruct_func)(i) for i in tqdm(range(min(maxLimit, training_df.shape[0]))))\n",
    "#     json.dump(logs, open(f'./dataset/WikiTableQuestions/GptInstruct_results_{split}.json', 'w'))\n",
    "\n",
    "# for split in splits:\n",
    "#     training_df = pd.read_csv(f'./dataset/WikiTableQuestions/data/{split}.tsv', sep='\\t', on_bad_lines='warn')\n",
    "#     print(training_df.shape)\n",
    "#     print(training_df.iloc[0])\n",
    "#     n_threads = 40\n",
    "#     from joblib import Parallel, delayed\n",
    "#     logs = Parallel(n_jobs=n_threads)(delayed(parallel_GptAnswerReason_func)(i) for i in tqdm(range(min(maxLimit, training_df.shape[0]))))\n",
    "#     json.dump(logs, open(f'./dataset/WikiTableQuestions/results/GptAnswerReason_results_{split}.json', 'w'))\n",
    "\n",
    "for split in splits:\n",
    "    training_df = pd.read_csv(f'./dataset/WikiTableQuestions/data/{split}.tsv', sep='\\t', on_bad_lines='warn')\n",
    "    print(training_df.shape)\n",
    "    print(training_df.iloc[0])\n",
    "    n_threads = 20\n",
    "    # maxLimit = 1\n",
    "    from joblib import Parallel, delayed\n",
    "    logs = Parallel(n_jobs=n_threads)(delayed(parallel_CodexAnswerReason_func)(i) for i in tqdm(range(min(maxLimit, training_df.shape[0]))))\n",
    "    json.dump(logs, open(f'./dataset/WikiTableQuestions/results/CodexAnswerReason_results_{split}.json', 'w'), indent=4)\n",
    "\n",
    "# for split in splits:\n",
    "#     training_df = pd.read_csv(f'./dataset/WikiTableQuestions/data/{split}.tsv', sep='\\t', on_bad_lines='warn')\n",
    "#     print(training_df.shape)\n",
    "#     print(training_df.iloc[0])\n",
    "#     n_threads = 20\n",
    "#     from joblib import Parallel, delayed\n",
    "#     logs = Parallel(n_jobs=n_threads)(delayed(parallel_CodexSQLReason_func)(i) for i in tqdm(range(min(maxLimit, training_df.shape[0]))))\n",
    "#     json.dump(logs, open(f'./dataset/WikiTableQuestions/results/CodexSQLReason_results_{split}.json', 'w'))\n",
    "\n",
    "# ft=None\n",
    "# training_df = pd.read_csv(f'./dataset/WikiTableQuestions/data/pristine-unseen-tables.tsv', sep='\\t', on_bad_lines='warn')\n",
    "# parallel_CodexAnswerOrderExplorer_func(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e0268a-806a-4b3d-bd50-924dd2a13296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b08087-e780-4883-8aaa-eaa42aee6222",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/mnt/text2sql/dataset/WikiTableQuestions/results/CodexAnswerOrderExplorer_results_pristine-unseen-tables.json', 'r') as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "with open(f'/mnt/text2sql/dataset/WikiTableQuestions/results/CodexAnswerOrderExplorer_results_pristine-unseen-tables_to_combine.json', 'r') as f:\n",
    "    logs_to_combine = json.load(f)\n",
    "\n",
    "assert len(logs) == len(logs_to_combine)\n",
    "for i in range(len(logs)):\n",
    "    assert logs[i]['id'] == logs_to_combine[i]['id']\n",
    "    if 'all_df_permutation_results' in logs[i] and 'all_df_permutation_results' in logs_to_combine[i]:\n",
    "        logs[i]['all_df_permutation_results'].update(logs_to_combine[i]['all_df_permutation_results'])\n",
    "\n",
    "with open(f'/mnt/text2sql/dataset/WikiTableQuestions/results/CodexAnswerOrderExplorer_results_pristine-unseen-tables_combined.json', 'w') as f:\n",
    "    json.dump(logs, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821cfeb5-27d4-428b-9b5b-081b8a423fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "553ecdf3-cdea-4519-b863-e85acf6e36db",
   "metadata": {},
   "source": [
    "# CodexAnswerOrderExplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e920212c-2cdb-49c5-b096-a781d4121c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14149/14149 [00:20<00:00, 703.18it/s]\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model('/mnt/idm_automapping/cc.en.300.bin')\n",
    "training_data_autoreasoning = json.load(open(f'./dataset/WikiTableQuestions/reasonings/GptReasoner_training.json', 'r'))\n",
    "all_demo_embeddings = None\n",
    "print(\"Loading embeddings\")\n",
    "for train_q in tqdm(training_data_autoreasoning):\n",
    "    if all_demo_embeddings is None:\n",
    "        all_demo_embeddings = get_utterance_embedding(train_q['utterance'], ft).reshape([-1, 1])\n",
    "    else:\n",
    "        all_demo_embeddings = np.append(all_demo_embeddings, get_utterance_embedding(train_q['utterance'], ft).reshape([-1, 1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2742415-4f22-4adb-a866-fcf5c88f9ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4344, 4)\n",
      "id                                                          nu-0\n",
      "utterance      which country had the most cyclists finish wit...\n",
      "context                                      csv/203-csv/733.csv\n",
      "targetValue                                                Italy\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4344/4344 [28:59<00:00,  2.50it/s]\n"
     ]
    }
   ],
   "source": [
    "def parallel_CodexAnswerOrderExplorer_func(i):\n",
    "    try:\n",
    "        prompter = CodexAnswerOrderExplorer(\n",
    "            training_df.iloc[i]['id'], \n",
    "            training_df.iloc[i]['utterance'], \n",
    "            training_df.iloc[i]['context'], \n",
    "            training_df.iloc[i]['targetValue'], \n",
    "            base_path='./dataset/WikiTableQuestions/'\n",
    "        )\n",
    "        prompter._gen_all_table_permutations(ft=ft)\n",
    "        prompter._gen_NN_demo(training_data_autoreasoning, all_demo_embeddings, ft, demo_num=3)\n",
    "        prompter._explore_all_dataframe_permutations()\n",
    "        return prompter._log_dict()\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'id': training_df.iloc[i]['id'],\n",
    "            'uncaught_err': str(e)\n",
    "        }\n",
    "    \n",
    "def parallel_CodexAnswerRandShuffle_func(i):\n",
    "    try:\n",
    "        prompter = CodexAnswerRandShuffle(\n",
    "            training_df.iloc[i]['id'], \n",
    "            training_df.iloc[i]['utterance'], \n",
    "            training_df.iloc[i]['context'], \n",
    "            training_df.iloc[i]['targetValue'], \n",
    "            base_path='./dataset/WikiTableQuestions/'\n",
    "        )\n",
    "        prompter._gen_all_table_permutations(ft=ft)\n",
    "        prompter._gen_NN_demo(training_data_autoreasoning, all_demo_embeddings, ft, demo_num=3)\n",
    "        prompter._explore_all_dataframe_permutations()\n",
    "        return prompter._log_dict()\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'id': training_df.iloc[i]['id'],\n",
    "            'uncaught_err': str(e)\n",
    "        }\n",
    "    \n",
    "maxLimit = np.inf \n",
    "# maxLimit = 20\n",
    "splits = ['pristine-unseen-tables']\n",
    "n_threads = 20\n",
    "\n",
    "# for split in splits:\n",
    "#     training_df = pd.read_csv(f'./dataset/WikiTableQuestions/data/{split}.tsv', sep='\\t', on_bad_lines='warn')\n",
    "#     print(training_df.shape)\n",
    "#     print(training_df.iloc[0])\n",
    "#     from joblib import Parallel, delayed\n",
    "#     logs = Parallel(n_jobs=n_threads, require='sharedmem')(delayed(parallel_CodexAnswerOrderExplorer_func)(i) for i in tqdm(range(min(maxLimit, training_df.shape[0]))))\n",
    "#     json.dump(logs, open(f'./dataset/WikiTableQuestions/results/CodexAnswerOrderExplorer_results_{split}_complete.json', 'w'))\n",
    "\n",
    "for split in splits:\n",
    "    training_df = pd.read_csv(f'./dataset/WikiTableQuestions/data/{split}.tsv', sep='\\t', on_bad_lines='warn')\n",
    "    print(training_df.shape)\n",
    "    print(training_df.iloc[0])\n",
    "    from joblib import Parallel, delayed\n",
    "    logs = Parallel(n_jobs=n_threads, require='sharedmem')(delayed(parallel_CodexAnswerRandShuffle_func)(i) for i in tqdm(range(min(maxLimit, training_df.shape[0]))))\n",
    "    json.dump(logs, open(f'./dataset/WikiTableQuestions/results/CodexAnswerRandShuffle_func_results_{split}_complete.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e491b0-f71c-4658-aea7-ccfd9c101814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a605e9a-6845-47ff-b761-91f529d5114c",
   "metadata": {},
   "source": [
    "# cos few shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07a48570-4220-445a-9409-48410c876740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14149/14149 [00:11<00:00, 1265.07it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4344/4344 [45:09<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# model = 'text-davinci-002'\n",
    "# model = 'text-davinci-003'\n",
    "model = 'davinci-codex-002-msft'\n",
    "\n",
    "ft = fasttext.load_model('/mnt/idm_automapping/cc.en.300.bin')\n",
    "\n",
    "def parallel_gpt_answer_dynamic_few_shot_func(i):\n",
    "    try:\n",
    "        prompter = GptAnswer(test_df.iloc[i]['id'], \n",
    "                             test_df.iloc[i]['utterance'], \n",
    "                             test_df.iloc[i]['context'], \n",
    "                             test_df.iloc[i]['targetValue'], \n",
    "                             base_path='./dataset/WikiTableQuestions/')\n",
    "        prompter.model = model\n",
    "        prompter._gen_NN_demo(training_data_autoreasoning, all_demo_embeddings, ft, demo_num=3)\n",
    "        prompter._gen_gpt_prompt()\n",
    "        prompter._get_gpt_prediction()\n",
    "        if prompter.gpt_error is None:\n",
    "            prompter._evaluate_result(verbose=False)\n",
    "        log = prompter._log_dict()\n",
    "    except Exception as e:\n",
    "        log = {\n",
    "            'id': test_df.iloc[i]['id'],\n",
    "            'uncaught_err': str(e)\n",
    "        }\n",
    "    if model == 'text-davinci-003':\n",
    "        time.sleep(5)\n",
    "    return log\n",
    "\n",
    "training_data_autoreasoning = json.load(open(f'./dataset/WikiTableQuestions/reasonings/GptReasoner_training.json', 'r'))\n",
    "all_demo_embeddings = None\n",
    "print(\"Loading embeddings\")\n",
    "for train_q in tqdm(training_data_autoreasoning):\n",
    "    if all_demo_embeddings is None:\n",
    "        all_demo_embeddings = get_utterance_embedding(train_q['utterance'], ft).reshape([-1, 1])\n",
    "    else:\n",
    "        all_demo_embeddings = np.append(all_demo_embeddings, get_utterance_embedding(train_q['utterance'], ft).reshape([-1, 1]), axis=1)\n",
    "\n",
    "test_df = pd.read_csv(f'./dataset/WikiTableQuestions/data/pristine-unseen-tables.tsv', sep='\\t', on_bad_lines='skip')\n",
    "maxLimit = float('inf')\n",
    "# maxLimit = 30\n",
    "n_threads = 1\n",
    "from joblib import Parallel, delayed\n",
    "res = Parallel(n_jobs=n_threads, require='sharedmem')(delayed(parallel_gpt_answer_dynamic_few_shot_func)(i) for i in tqdm(range(min(maxLimit, test_df.shape[0]))))\n",
    "json.dump(res, open(f'./dataset/WikiTableQuestions/results/GptAnswer_{model}_fewShot_top3cos_results_pristine-unseen-tables.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4264897-2ef3-4f6c-8036-8216b82c4c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = pd.read_csv(f'./dataset/WikiTableQuestions/data/pristine-unseen-tables.tsv', sep='\\t', on_bad_lines='skip')\n",
    "# parallel_gpt_answer_dynamic_few_shot_func(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519ca371-0701-473c-a945-74735a267367",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens, however you requested 7490 tokens (7362 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "Execution match: 0.46\n",
      "Execution err: 0.00\n",
      "GPT error: 0.02\n"
     ]
    }
   ],
   "source": [
    "execution_match_cnt = 0\n",
    "execution_err_cnt = 0\n",
    "gpt_error = 0\n",
    "\n",
    "for i, q in enumerate(logs):\n",
    "    if 'execution_match' in q and q['execution_match']==True:\n",
    "        execution_match_cnt += 1\n",
    "    if 'execution_err' in q and q['execution_err'] is not None:\n",
    "        execution_err_cnt += 1\n",
    "    if 'gpt_error' in q and q['gpt_error'] is not None:\n",
    "        gpt_error += 1\n",
    "        # print(i)\n",
    "        if gpt_error == 1:\n",
    "            print(q['gpt_error'])\n",
    "    # if 'target_value' in q and q['predicted_value'] is not None and q['target_value'] != q['predicted_value'] and q['target_value'] in q['predicted_value']:\n",
    "    #     print(f\"Prediction: {q['predicted_value']}, target: {q['target_value']}\")\n",
    "print(f\"Execution match: {execution_match_cnt/len(logs):.2f}\")\n",
    "print(f\"Execution err: {execution_err_cnt/len(logs):.2f}\")\n",
    "if gpt_error > 0:\n",
    "    print(f\"GPT error: {gpt_error/len(logs):.2f}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49668dac-ce10-4dc6-b8ee-0ded6ab52047",
   "metadata": {},
   "source": [
    "# Write prediction file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe8369-0042-47f9-9213-2fc419c4618f",
   "metadata": {},
   "source": [
    "## single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e776648-fd6b-44be-9000-81eb8cd216a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Dataset version: \"Version 1.0.2 (October 4, 2016)\"\\nusing test split: pristine-unseen-tables.tsv\\nReading split file:  pristine-unseen-tables.tsv\\nRead 4344 example IDs\\nExample ids:  [\\'nu-0\\', \\'nu-1\\', \\'nu-2\\', \\'nu-3\\', \\'nu-4\\', \\'nu-5\\', \\'nu-6\\', \\'nu-7\\', \\'nu-8\\', \\'nu-9\\']\\nUsing prediction file:  ./dataset/WikiTableQuestions/results/GptAnswer_davinci-codex-002-msft_fewShot_top3cos_results_pristine-unseen-tables_predictions.tsv\\nReading dataset from ./dataset/WikiTableQuestions/tagged/data/training.tagged\\nReading dataset from ./dataset/WikiTableQuestions/tagged/data/pristine-seen-tables.tagged\\nReading dataset from ./dataset/WikiTableQuestions/tagged/data/pristine-unseen-tables.tagged\\nRead 22033 examples\\nReading predictions from ./dataset/WikiTableQuestions/results/GptAnswer_davinci-codex-002-msft_fewShot_top3cos_results_pristine-unseen-tables_predictions.tsv\\nExamples: 4344\\nCorrect: 893\\nAccuracy: 0.2056\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "with open(f'./dataset/WikiTableQuestions/results/GptAnswer_{model}_fewShot_top3cos_results_pristine-unseen-tables.json', 'r') as f:\n",
    "    logs = json.load(f)\n",
    "predictions = ''\n",
    "for q in logs:\n",
    "    qid = q['id'].replace(' ', '')\n",
    "    if 'predicted_value' in q:\n",
    "        result = str(q['predicted_value']).replace('\\n', ' ').replace('\\t', ' ')\n",
    "    else:\n",
    "        result = ''\n",
    "    predictions += f\"{qid}\\t{result}\\n\"\n",
    "with open(f'./dataset/WikiTableQuestions/results/GptAnswer_{model}_fewShot_top3cos_results_pristine-unseen-tables_predictions.tsv', 'w') as g:\n",
    "    g.write(predictions)\n",
    "\n",
    "# import subprocess\n",
    "# out = subprocess.check_output(['python2', './dataset/WikiTableQuestions/validate-and-evaluate.py', \n",
    "#                                '-d', './dataset/WikiTableQuestions/', \n",
    "#                                '-o', './dataset/WikiTableQuestions/results/output.json',\n",
    "#                                'test', \n",
    "#                                f'./dataset/WikiTableQuestions/results/GptAnswer_{model}_fewShot_top3cos_results_pristine-unseen-tables_predictions.tsv'])\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c342e5d-08f8-49b0-8932-122b39d2c31a",
   "metadata": {},
   "source": [
    "## multiple predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0a34f83-fd9e-4951-9be0-6d8def919eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/mnt/text2sql/dataset/WikiTableQuestions/results/CodexAnswerOrderExplorer_results_pristine-unseen-tables.json', 'r') as f:\n",
    "    logs = json.load(f)\n",
    "predictions = ''\n",
    "for q in logs:\n",
    "    qid = q['id'].replace(' ', '')\n",
    "    predictions += f'{qid}'\n",
    "    if 'all_df_permutation_results' in q:\n",
    "        for reason in q['all_df_permutation_results']:\n",
    "            result = q['all_df_permutation_results'][reason]\n",
    "            predictions += f\"\\t{result}\"\n",
    "    predictions += '\\n'\n",
    "with open(f'./dataset/WikiTableQuestions/results/CodexAnswerOrderExplorer_results_pristine-unseen-tables_predictions.tsv', 'w') as g:\n",
    "    g.write(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c510be63-abdb-4420-a81a-3ff9b8aad178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'nu-228',\n",
       " 'uncaught_err': 'Error tokenizing data. C error: EOF inside string starting at row 11'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1705b555-92c8-42ca-912f-56c28fb7aa53",
   "metadata": {},
   "source": [
    "# Result quality summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b9d8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 8193 tokens, however you requested 10149 tokens (8149 in your prompt; 2000 for the completion). Please reduce your prompt; or completion length.\n",
      "CodexSQL_fewShot: Execution match: 0.49\n",
      "Execution err: 0.08\n",
      "Execution err: 0.02\n",
      "This model's maximum context length is 4097 tokens, however you requested 8133 tokens (8005 in your prompt; 128 for the completion). Please reduce your prompt; or completion length.\n",
      "GptAnswer_fewShot: Execution match: 0.46\n",
      "Execution err: 0.00\n",
      "Execution err: 0.04\n"
     ]
    }
   ],
   "source": [
    "['CodexSQL', 'CodexSQL_fewShot', 'GptAnswer', 'GptAnswer_fewShot', 'GptAnswerReason', 'CodexSQLReason']\n",
    "for method in ['CodexSQL_fewShot', 'GptAnswer_fewShot']:\n",
    "    split = 'pristine-unseen-tables'\n",
    "    with open(f'./dataset/WikiTableQuestions/results/{method}_demoNum{demo_num}_atIndex{at_index}_results_{split}.json', 'r') as f:\n",
    "        logs = json.load(f)\n",
    "    execution_match_cnt = 0\n",
    "    execution_err_cnt = 0\n",
    "    gpt_error = 0\n",
    "    for q in logs:\n",
    "        if 'execution_match' in q and q['execution_match']==True:\n",
    "            execution_match_cnt += 1\n",
    "        if 'execution_err' in q and q['execution_err'] is not None:\n",
    "            execution_err_cnt += 1\n",
    "        if 'gpt_error' in q and q['gpt_error'] is not None:\n",
    "            gpt_error += 1\n",
    "            if gpt_error == 1:\n",
    "                print(q['gpt_error'])\n",
    "    print(f\"{method}: Execution match: {execution_match_cnt/len(logs):.2f}\")\n",
    "    print(f\"Execution err: {execution_err_cnt/len(logs):.2f}\")\n",
    "    if gpt_error > 0:\n",
    "        print(f\"GPT error: {gpt_error/len(logs):.2f}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b6d042-59a0-41a3-bd02-bf158eab9434",
   "metadata": {},
   "source": [
    "# format prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "938d48f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 264 (char 263)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m splits:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset/WikiTableQuestions/results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_demoNum\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdemo_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_atIndex\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mat_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 8\u001b[0m         logs \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m logs:\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 264 (char 263)"
     ]
    }
   ],
   "source": [
    "configs = ['CodexSQL', 'CodexSQL_fewShot', 'GptAnswer', 'GptAnswer_fewShot', 'GptAnswerReason', 'CodexSQLReason']\n",
    "configs = ['CodexSQL_fewShot', 'GptAnswer_fewShot']\n",
    "\n",
    "import subprocess\n",
    "for method in configs:\n",
    "    for split in splits:\n",
    "        with open(f'./dataset/WikiTableQuestions/results/{method}_demoNum{demo_num}_atIndex{at_index}_results_{split}.json', 'r') as f:\n",
    "            logs = json.load(f)\n",
    "        predictions = ''\n",
    "        for q in logs:\n",
    "            qid = q['id'].replace(' ', '')\n",
    "            if 'predicted_value' in q:\n",
    "                result = str(q['predicted_value']).replace('\\n', ' ').replace('\\t', ' ')\n",
    "            else:\n",
    "                result = ''\n",
    "            predictions += f\"{qid}\\t{result}\\n\"\n",
    "        with open(f'./dataset/WikiTableQuestions/results/{method}_demoNum{demo_num}_atIndex{at_index}_{split}_predictions.tsv', 'w') as g:\n",
    "            g.write(predictions)\n",
    "    \n",
    "    # cmd = ' '.join(['python2', './dataset/WikiTableQuestions/validate-and-evaluate.py', '-d', './dataset/WikiTableQuestions/', 'test', \n",
    "    #                                f'./dataset/WikiTableQuestions/results/{method}_demoNum{demo_num}_atIndex{at_index}_results_{split}.json'])\n",
    "    # cmd = ' '.join(['python2', 'validate-and-evaluate.py', '-d', './', 'test', \n",
    "    #                                f'./results/{method}_demoNum{demo_num}_atIndex{at_index}_{split}_predictions.tsv'])\n",
    "    # print(cmd)\n",
    "    out = subprocess.check_output(['python2', './dataset/WikiTableQuestions/validate-and-evaluate.py', '-d', './dataset/WikiTableQuestions/', 'test', \n",
    "                                   f'./dataset/WikiTableQuestions/results/{method}_demoNum{demo_num}_atIndex{at_index}_{split}_predictions.tsv'])\n",
    "    print(method)\n",
    "    print(out)\n",
    "    # python2 validate-and-evaluate.py -d ./ test ./results/CodexSQL_fewShot_pristine-unseen-tables_predictions.tsv\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91ff08-f035-410c-91a4-9c9f1a233619",
   "metadata": {},
   "source": [
    "# Combine ensumble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e6aaba6-7cd8-4df7-a9fd-cea2ba0268ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectMajority(l):\n",
    "    from collections import Counter\n",
    "    c = Counter(l)\n",
    "    value, _ = c.most_common()[0]\n",
    "    return value\n",
    "\n",
    "configs = ['GptAnswer_fewShot_demoNumNone_atIndex{}', 'CodexSQL_fewShot_demoNumNone_atIndex{}']\n",
    "for method in configs:\n",
    "    answer_dicts = []\n",
    "    for index in [0, 1, 2,]:\n",
    "        answer_dicts.append(json.load(open(f'./dataset/WikiTableQuestions/results/{method.format(index)}_results_pristine-unseen-tables.json', 'r')))\n",
    "    predictions = ''\n",
    "    for i, q in enumerate(answer_dicts[0]):\n",
    "        qid = q['id'].replace(' ', '')\n",
    "        current_predictions = []\n",
    "        for d in answer_dicts:\n",
    "            if 'predicted_value' in d[i]:\n",
    "                current_predictions.append(str(d[i]['predicted_value']).replace('\\n', ' ').replace('\\t', ' '))\n",
    "            else:\n",
    "                current_predictions = ['']\n",
    "        # print(selectMajority(current_predictions))\n",
    "        predictions += f\"{qid}\\t{selectMajority(current_predictions)}\\n\"\n",
    "    with open(f'./dataset/WikiTableQuestions/results/{method}_ensumble_pristine-unseen-tables_predictions.tsv', 'w') as g:\n",
    "        g.write(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6730d24c-77d9-4967-b192-1f14eff124d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86dae4a-41d9-4104-b76a-92687c7a4c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
