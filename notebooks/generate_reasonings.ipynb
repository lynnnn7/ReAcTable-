{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3423d150-219f-4409-9262-f3d1e0fbcf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52835e54-bf81-4d6d-a209-6b6310854c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "from gpt3_sandbox.api.gpt import GPT\n",
    "from gpt3_sandbox.api.gpt import Example\n",
    "from pandasql import sqldf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from AutoReasoner import *\n",
    "\n",
    "import fasttext.util\n",
    "# download the fastext embeding model if needed \n",
    "# fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('/mnt/idm_automapping/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb5853-329b-40b5-b722-b0ca0edbcc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe806d29-5616-437f-8879-c41b5a732a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ab41dc-4006-4f1a-811c-5499ac1b314a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14149, 4)\n",
      "id                                                          nt-0\n",
      "utterance      what was the last year where this team was a p...\n",
      "context                                      csv/204-csv/590.csv\n",
      "targetValue                                                 2004\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                                 | 40/14149 [00:05<30:17,  7.76it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[0;32m---> 32\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparallel_gpt_reasoner_func\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmaxLimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                                 | 40/14149 [00:19<30:17,  7.76it/s]"
     ]
    }
   ],
   "source": [
    "def parallel_gpt_reasoner_func(i):\n",
    "    try:\n",
    "        prompter = GptReasoner(\n",
    "            training_df.iloc[i]['id'], \n",
    "            training_df.iloc[i]['utterance'], \n",
    "            training_df.iloc[i]['context'], \n",
    "            training_df.iloc[i]['targetValue'], \n",
    "            base_path='./dataset/WikiTableQuestions/',\n",
    "        )\n",
    "        prompter.generate_reasonings()\n",
    "        return {\n",
    "            'id': training_df.iloc[i]['id'],\n",
    "            'utterance': training_df.iloc[i]['utterance'],\n",
    "            'context': training_df.iloc[i]['context'], \n",
    "            'targetValue': training_df.iloc[i]['targetValue'],\n",
    "            'gptReasoning': prompter.gpt_reasoning,\n",
    "            'gptError': prompter.gpt_error\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'id': training_df.iloc[i]['id'],\n",
    "            'uncaught_err': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "maxLimit = float('inf')\n",
    "training_df = pd.read_csv(f'./dataset/WikiTableQuestions/data/training.tsv', sep='\\t', on_bad_lines='warn')\n",
    "print(training_df.shape)\n",
    "print(training_df.iloc[0])\n",
    "n_threads = 10\n",
    "from joblib import Parallel, delayed\n",
    "logs = Parallel(n_jobs=n_threads)(delayed(parallel_gpt_reasoner_func)(i) for i in tqdm(range(min(maxLimit, training_df.shape[0]))))\n",
    "# # logs = Parallel(n_jobs=n_threads)(delayed(parallel_gpt_answer_func)(i) for i in tqdm(range(5)))\n",
    "# json.dump(logs, open(f'./dataset/WikiTableQuestions/reasonings/GptReasoner_training.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb663921-6750-4e42-9ee2-cc6a42fa63c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7de292d-688b-4372-b8de-e5e1758e93c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74056a1b-c54f-4739-9403-f8295ae9411e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14149/14149 [00:18<00:00, 753.82it/s]\n"
     ]
    }
   ],
   "source": [
    "training_data_autoreasoning = json.load(open(f'./dataset/WikiTableQuestions/reasonings/GptReasoner_training.json', 'r'))\n",
    "all_demo_embeddings = None\n",
    "for train_q in tqdm(training_data_autoreasoning):\n",
    "    if all_demo_embeddings is None:\n",
    "        all_demo_embeddings = get_utterance_embedding(train_q['utterance'], ft).reshape([-1, 1])\n",
    "    else:\n",
    "        all_demo_embeddings = np.append(all_demo_embeddings, get_utterance_embedding(train_q['utterance'], ft).reshape([-1, 1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f38cea8-7e06-4a06-8b74-79a1253493c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 14149)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_demo_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e659dc02-d7db-4dbb-a1a8-fbd81bf00235",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████████████████████████████████                                                                                           | 1280/4344 [04:53<12:12,  4.18it/s]"
     ]
    }
   ],
   "source": [
    "def parallel_auto_reasoner_func(i):\n",
    "    try:\n",
    "        r = GptAutoReasoner(\n",
    "            test_df.iloc[i]['id'], \n",
    "            test_df.iloc[i]['utterance'], \n",
    "            test_df.iloc[i]['context'], \n",
    "            test_df.iloc[i]['targetValue'] )\n",
    "        r._gen_NN_demo(training_data_autoreasoning, all_demo_embeddings, ft, demo_num=3)\n",
    "        r._gen_gpt_prompt()\n",
    "        r._get_gpt_prediction()\n",
    "        r._evaluate_result()\n",
    "        return r._log_dict()\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'id': test_df.iloc[i]['id'],\n",
    "            'uncaught_err': str(e)\n",
    "        }\n",
    "\n",
    "test_df = pd.read_csv(f'./dataset/WikiTableQuestions/data/pristine-unseen-tables.tsv', sep='\\t', on_bad_lines='skip')\n",
    "maxLimit = float('inf')\n",
    "# maxLimit = 20\n",
    "n_threads = 20\n",
    "from joblib import Parallel, delayed\n",
    "res = Parallel(n_jobs=n_threads, require='sharedmem')(delayed(parallel_auto_reasoner_func)(i) for i in tqdm(range(min(maxLimit, test_df.shape[0]))))\n",
    "json.dump(res, open(f'./dataset/WikiTableQuestions/results/AutoReasoner_pristine-unseen-tables_demoNum3.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f2baad0-e8d8-4ebc-9d4c-d704b6594c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 8193 tokens, however you requested 12417 tokens (11393 in your prompt; 1024 for the completion). Please reduce your prompt; or completion length.\n",
      "Execution match: 0.51\n",
      "Execution err: 0.00\n",
      "GPT error: 0.09\n"
     ]
    }
   ],
   "source": [
    "with open(f'./dataset/WikiTableQuestions/results/AutoReasoner_pristine-unseen-tables_demoNum3.json', 'r') as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "    execution_match_cnt = 0\n",
    "execution_err_cnt = 0\n",
    "gpt_error = 0\n",
    "\n",
    "for q in logs:\n",
    "    \n",
    "    if 'execution_match' in q and q['execution_match']==True:\n",
    "        execution_match_cnt += 1\n",
    "    if 'execution_err' in q and q['execution_err'] is not None:\n",
    "        execution_err_cnt += 1\n",
    "    if 'gpt_error' in q and q['gpt_error'] is not None:\n",
    "        gpt_error += 1\n",
    "        if gpt_error == 1:\n",
    "            print(q['gpt_error'])\n",
    "            \n",
    "    # if 'target_value' in q and q['target_value'] != q['predicted_value'] and q['target_value'] in q['predicted_value']:\n",
    "    #     if gpt_error == 0:\n",
    "    #         print(f\"Prediction: {q['predicted_value']}, target: {q['target_value']}\")\n",
    "    #     gpt_error += 1\n",
    "    #     pass\n",
    "    \n",
    "print(f\"Execution match: {execution_match_cnt/len(logs):.2f}\")\n",
    "print(f\"Execution err: {execution_err_cnt/len(logs):.2f}\")\n",
    "if gpt_error > 0:\n",
    "    print(f\"GPT error: {gpt_error/len(logs):.2f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4c287ec-f701-4417-ab5e-60f2bc31e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./dataset/WikiTableQuestions/results/AutoReasoner_pristine-unseen-tables_demoNum3.json', 'r') as f:\n",
    "    logs = json.load(f)\n",
    "predictions = ''\n",
    "for q in logs:\n",
    "    qid = q['id'].replace(' ', '')\n",
    "    if 'predicted_value' in q:\n",
    "        result = str(q['predicted_value']).replace('\\n', ' ').replace('\\t', ' ')\n",
    "    else:\n",
    "        result = ''\n",
    "    predictions += f\"{qid}\\t{result}\\n\"\n",
    "with open(f'./dataset/WikiTableQuestions/results/AutoReasoner_pristine-unseen-tables_demoNum3_predictions.tsv', 'w') as g:\n",
    "    g.write(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae8b5247-77a7-4f8b-b336-4d954c4d19dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Dataset version: \"Version 1.0.2 (October 4, 2016)\"\\nusing test split: pristine-unseen-tables.tsv\\nReading split file:  pristine-unseen-tables.tsv\\nRead 4344 example IDs\\nExample ids:  [\\'nu-0\\', \\'nu-1\\', \\'nu-2\\', \\'nu-3\\', \\'nu-4\\', \\'nu-5\\', \\'nu-6\\', \\'nu-7\\', \\'nu-8\\', \\'nu-9\\']\\nUsing prediction file:  ./dataset/WikiTableQuestions/results/AutoReasoner_pristine-unseen-tables_demoNum3_predictions.tsv\\nReading dataset from ./dataset/WikiTableQuestions/tagged/data/training.tagged\\nReading dataset from ./dataset/WikiTableQuestions/tagged/data/pristine-seen-tables.tagged\\nReading dataset from ./dataset/WikiTableQuestions/tagged/data/pristine-unseen-tables.tagged\\nRead 22033 examples\\nReading predictions from ./dataset/WikiTableQuestions/results/AutoReasoner_pristine-unseen-tables_demoNum3_predictions.tsv\\nExamples: 4344\\nCorrect: 2168\\nAccuracy: 0.4991\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "out = subprocess.check_output(['python2', './dataset/WikiTableQuestions/validate-and-evaluate.py', \n",
    "                               '-d', './dataset/WikiTableQuestions/', \n",
    "                               '-o', './dataset/WikiTableQuestions/results/output.json',\n",
    "                               'test', \n",
    "                               f'./dataset/WikiTableQuestions/results/AutoReasoner_pristine-unseen-tables_demoNum3_predictions.tsv'])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27f367-637d-4a1b-87d0-4044294b77a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e6ad51-93e5-47a2-83e3-d2ddb3d61422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faab048-349a-47eb-9b90-c582775c7a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f62dc3-3c16-4fd1-a97c-becf8786b8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88c5f669-2b06-4fad-bd4e-71544a110902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Goodbye\" in Chinese is \"再见\"\n"
     ]
    }
   ],
   "source": [
    "print(openai.Completion.create(engine='text-davinci-002',\n",
    "                                            prompt='What is \"goodbye\" in Chinese?',\n",
    "                                ).choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0917623-f202-45b8-9411-256e16e8525f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
