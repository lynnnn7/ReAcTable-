{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7fa0fb0-dbd8-4ec5-8b5d-b9c237455edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c54b74f4-099c-4f47-89f2-305d54d6c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import openai\n",
    "# import os\n",
    "import json\n",
    "# from gpt3_sandbox.api.gpt import GPT\n",
    "# from gpt3_sandbox.api.gpt import Example\n",
    "from pandasql import sqldf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from GptPrompter import *\n",
    "from GptCOTPrompter import *\n",
    "from AutoReasoner import *\n",
    "import dotenv\n",
    "\n",
    "config = dotenv.dotenv_values(\".env\")\n",
    "openai.api_type = 'azure'\n",
    "openai.api_base = 'https://meta-prompter-az-openai.openai.azure.com'\n",
    "openai.api_version = '2022-12-01'\n",
    "openai.api_key = config['OPENAI_API_KEY_ms']\n",
    "\n",
    "dataset = pd.read_csv('./dataset/FeTaQA/test.csv', sep=',')\n",
    "# dataset = pd.read_csv('./dataset/FeTaQA/test_sample400.csv', sep=',')\n",
    "\n",
    "ft = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e5638f-b4df-4d68-ab11-a5392626a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_df = dataset.sample(frac=0.2, random_state=0)\n",
    "# print(sampled_df.shape)\n",
    "# sampled_df.to_csv('./dataset/FeTaQA/test_sample400.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "793811e9-8737-4d8f-bcbb-274e539de27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# import fasttext\n",
    "# ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3213c2d0-01ac-4508-b553-0a58a2ecb067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7f7e375-ef7d-40ae-b0c8-0551d93e6c41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████████████████████                                                                                                                                                                                                                    | 204/2006 [03:21<26:03,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: string indices must be integers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                               | 788/2006 [12:52<21:22,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to SQLite for execution code: SELECT ABS(111.688 - 111.470) AS diff FROM DF;.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                          | 1778/2006 [29:34<03:36,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: string indices must be integers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊        | 1936/2006 [32:06<01:21,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to SQLite for execution code: SELECT ROUND(votes, 1) FROM DF;.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2006/2006 [33:15<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "NNDemo = False\n",
    "max_demo = 5\n",
    "\n",
    "# template = 'original-sql-py'\n",
    "template = 'original-sql-py-no-intermediate'\n",
    "# template = 'formatv1-sql-py'\n",
    "\n",
    "# gpt_model = 'text-davinci-003'\n",
    "# gpt_model = 'gpt-3.5-turbo'\n",
    "gpt_model = 'mp-aoi-codex'\n",
    "\n",
    "def parallel_codex_func_formatv1(i):\n",
    "    max_retry = 3\n",
    "    while max_retry>0:\n",
    "        try:\n",
    "            codex_prompter = CodexAnswerCOTExecutor_template(\n",
    "                                      f'prompt_template/{template}.json',\n",
    "                                      dataset.iloc[i]['id'], \n",
    "                                      dataset.iloc[i]['utterance'], \n",
    "                                      dataset.iloc[i]['context'], \n",
    "                                      dataset.iloc[i]['targetValue'],  \n",
    "                                      base_path='./dataset/FeTaQA/',\n",
    "                                      demo_file=f'few-shot-demo/FeTaQA-{program}.json',\n",
    "                                     )\n",
    "            codex_prompter.model = gpt_model\n",
    "            codex_prompter.max_demo = max_demo\n",
    "            # codex_prompter.demo_ids = [0, 1, 2, 3, 6, 8, 11]\n",
    "            \n",
    "            # codex_prompter._gen_gpt_prompt()\n",
    "            codex_prompter._gen_gpt_prompt(NNDemo, ft)\n",
    "            codex_prompter._get_gpt_prediction()\n",
    "            log = codex_prompter._log_dict()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            log = {\n",
    "                'id': dataset.iloc[i]['id'],\n",
    "                'uncaught_err': str(e)\n",
    "            }\n",
    "            if \"model's maximum context length\" in str(e):\n",
    "                return log\n",
    "            max_retry -= 1\n",
    "    return log\n",
    "    \n",
    "for program in ['sql-py']:\n",
    "    n_threads = 2\n",
    "    maxLimit = float('inf')\n",
    "    # maxLimit = 100\n",
    "    from joblib import Parallel, delayed\n",
    "    output_result_file = f'./dataset/FeTaQA/results/CodexAnswerCOTExecutor_{template}_{program}_NNDemo={NNDemo}_results_pristine-unseen-tables_limit{maxLimit}_model{gpt_model}.json'\n",
    "    logs = Parallel(n_jobs=n_threads, require='sharedmem')(delayed(parallel_codex_func_formatv1)(i) for i in tqdm(range(min(maxLimit, dataset.shape[0]))))\n",
    "    json.dump(logs, open(output_result_file, 'w'), indent=4)\n",
    "    # evaluate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df0cf0bc-91d2-4e39-a980-60cc08b245cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.6473680302800984,\n",
       " 'rouge2': 0.41243101776713587,\n",
       " 'rougeL': 0.5450562792752759}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "BLEUs = []\n",
    "\n",
    "all_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "BLEU = 0\n",
    "for l in logs:\n",
    "    if not 'predicted_value' in l:\n",
    "        continue\n",
    "    # scores = scorer.score(l['target_value'], l['predicted_value'])\n",
    "    scores = scorer.score(l['target_value'], l['utterance'] + ' ' + l['predicted_value'])\n",
    "    # scores = scorer.score(l['predicted_value'], l['target_value'])\n",
    "    \n",
    "    # print(scores)\n",
    "    for m in scores:\n",
    "        all_scores[m].append(scores[m].recall)\n",
    "        # all_scores[m].append(scores[m].precision)\n",
    "    \n",
    "#     # candidate_tokens = nltk.word_tokenize(l['predicted_value'].lower())\n",
    "#     # reference_tokens = nltk.word_tokenize(l['target_value'].lower())\n",
    "#     candidate_tokens = l['predicted_value'].split()\n",
    "#     reference_tokens = l['target_value'].split()\n",
    "    \n",
    "#     BLEUs.append(nltk.translate.bleu_score.sentence_bleu([reference_tokens], candidate_tokens))\n",
    "    \n",
    "    # break\n",
    "all_scores = {'rouge1': np.mean(all_scores['rouge1']), 'rouge2': np.mean(all_scores['rouge2']), 'rougeL': np.mean(all_scores['rougeL'])}\n",
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc08ed70-2e4e-4dbc-97ef-62d17802ea57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.20958185489849235\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def calculate_bleu_score(predicted_text, reference_text):\n",
    "    # print(predicted_text)\n",
    "    # print(reference_text)\n",
    "    smoothie = nltk.translate.bleu_score.SmoothingFunction().method4\n",
    "    bleu_score = nltk.translate.bleu_score.sentence_bleu(\n",
    "        [reference_text.split()],                                                  \n",
    "        predicted_text.split(),\n",
    "        smoothing_function=smoothie\n",
    "    )\n",
    "    # print(bleu_score)\n",
    "    return bleu_score\n",
    "\n",
    "BLEU = 0\n",
    "BLEUs = []\n",
    "for l in logs:\n",
    "    if not 'predicted_value' in l:\n",
    "        continue\n",
    "    \n",
    "    BLEUs.append(calculate_bleu_score(l['predicted_value'], l['target_value']))\n",
    "    \n",
    "print(\"BLEU Score:\", np.average(BLEUs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c5f89e3-e41a-4b06-85b0-9cc4066553fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reference_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreference_corpus\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reference_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "reference_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcc71493-de7d-44a9-8bb0-000942e720e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NNDemo = False\n",
    "max_demo = 5\n",
    "template = 'original-sql-py-no-intermediate'\n",
    "program = 'sql-py'\n",
    "gpt_model = 'mp-aoi-codex'\n",
    "\n",
    "\n",
    "def func(i):\n",
    "    codex_prompter = CodexAnswerCOTExecutor_template(\n",
    "                                      f'prompt_template/{template}.json',\n",
    "                                      dataset.iloc[i]['id'], \n",
    "                                      dataset.iloc[i]['utterance'], \n",
    "                                      dataset.iloc[i]['context'], \n",
    "                                      dataset.iloc[i]['targetValue'],  \n",
    "                                      base_path='./dataset/FeTaQA/',\n",
    "                                      demo_file=f'few-shot-demo/FeTaQA-{program}.json',\n",
    "                                     )\n",
    "    codex_prompter.model = gpt_model\n",
    "    codex_prompter.max_demo = max_demo\n",
    "    # codex_prompter.demo_ids = [0, 2, 8, 11]\n",
    "\n",
    "    # codex_prompter._gen_gpt_prompt()\n",
    "    codex_prompter._gen_gpt_prompt(NNDemo, ft)\n",
    "    codex_prompter._get_gpt_prediction()\n",
    "    log = codex_prompter._log_dict()\n",
    "    # print(vars(codex_prompter))\n",
    "    # print(codex_prompter.frequency_penalty)\n",
    "    return log\n",
    "a = func(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "720442cb-7e13-4bcb-9b19-3f160a619447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 15555.0,\n",
       " 'utterance': 'What were the voting results of the 1975 North Vietnamese legislative election with regards to seats?',\n",
       " 'source_csv': './dataset/FeTaQA/totto_source/train_json/example-7854.csv',\n",
       " 'target_value': 'In the 1975 North Vietnamese legislative election, the Vietnamese Fatherland Front won all 424 seats.',\n",
       " 'predicted_value': 'The Vietnamese Fatherland Front won 424 seats.',\n",
       " 'prompt': 'The database table DF is shown as follows:\\n[HEAD]: aircraft|introduced|retired\\n---\\n[ROW] 1: Aérospatiale N 262|1967|1969\\n[ROW] 2: Boeing 707|-|-\\n[ROW] 3: Boeing 720|1976|1977\\n[ROW] 4: Convair 990 Coronado|1974|1975\\n[ROW] 5: de Havilland Comet|1962|1965\\n[ROW] 6: Douglas DC-3|1947|1976\\n[ROW] 7: Douglas DC-4|1949|1953\\n[ROW] 8: Douglas DC-8|1972|1978\\n...\\n[ROW] 14: Sud Aviation Caravelle|-|-\\n[ROW] 15: Vickers VC10|1965/1977|1971/1978\\n\\nAnswer the following question based on the data above: \"What Hawker planes has Air Ceylon had?\". Generate SQL or Python code step-by-step given the question and table to answer the question correctly. For each step, generate SQL code to process the query or Python code to reformat the data. Output the code braced by \"```\" and an external executor will process the code generated and feed an intermediate table back to you. Answer the question directly if confident.\\n\\nSQL: ```SELECT aircraft FROM DF WHERE aircraft LIKE \"%Hawker%\";```.\\n\\nIntermediate table:\\n[HEAD]: aircraft\\n---\\n[ROW] 1: Hawker Siddeley HS 748\\n[ROW] 2: Hawker Siddeley Trident\\n\\nAnswer: ```Air Ceylon had Hawker Siddeley HS 748 and Hawker Siddeley Trident.```.\\n\\nThe database table DF is shown as follows:\\n[HEAD]: party|party2|candidate|votes|percent\\n---\\n[ROW] 1: -|Democratic|Franklin D. Roosevelt (inc.)|31,945|60.08%\\n[ROW] 2: -|Republican|Wendell Willkie|21,229|39.92%\\n[ROW] 3: Total votes|Total votes|Total votes|53,174|100%\\n\\nAnswer the following question based on the data above: \"Which candidates ran in the 1940 United States presidential election in Nevada, and what percentage of the vote did each candidate earn?\". Generate SQL or Python code step-by-step given the question and table to answer the question correctly. For each step, generate SQL code to process the query or Python code to reformat the data. Output the code braced by \"```\" and an external executor will process the code generated and feed an intermediate table back to you. Answer the question directly if confident.\\n\\nSQL: ```SELECT candidate, percent FROM DF;```.\\n\\nIntermediate table:\\n[HEAD]: candidate|percent\\n---\\n[ROW] 1: Franklin D. Roosevelt (inc.)|60.08%\\n[ROW] 2: Wendell Willkie|39.92%\\n[ROW] 3: Total votes|100%\\n\\nAnswer: ```Nevada was won by incumbent Franklin D. Roosevelt (D), with 60.08% of the vote, against Wendell Willkie (R), with 39.92% of the vote.```.\\n\\nThe database table DF is shown as follows:\\n[HEAD]: year|title|role|notes\\n---\\n[ROW] 1: 2009–2013|We Speak NYC|Jorge / Fredy|\"Love and Money\" & \"The Storm\"\\n[ROW] 2: 2014–2019|Broad City|Jaimé Castro|-\\n[ROW] 3: 2015-2016|Alternatino|Arturo|Web series, 6 episodes\\n[ROW] 4: 2017|No Activity|Pedro|6 Episodes\\n[ROW] 5: 2017|Narcos|David Rodriguez|10 episodes\\n[ROW] 6: 2019|Alternatino|Arturo|-\\n\\nAnswer the following question based on the data above: \"What role did Arturo Castro play in Narcos?\". Generate SQL or Python code step-by-step given the question and table to answer the question correctly. For each step, generate SQL code to process the query or Python code to reformat the data. Output the code braced by \"```\" and an external executor will process the code generated and feed an intermediate table back to you. Answer the question directly if confident.\\n\\nSQL: ```SELECT * FROM DF WHERE title=\"Narcos\";```.\\n\\nIntermediate table:\\n[HEAD]: year|title|role|notes\\n---\\n[ROW] 1: 2017|Narcos|David Rodriguez|10 episodes\\n\\nAnswer: ```In 2017, Arturo played the role of David Rodriguez in Narcos.```.\\n\\nThe database table DF is shown as follows:\\n[HEAD]: party|party2|candidate|votes|percent|±\\n---\\n[ROW] 1: -|Liberal|Alexander Gordon Cummins Harvey|5,912|45.9|0.0\\n[ROW] 2: -|Conservative|Clement Royds|4,449|34.6|11.5\\n[ROW] 3: -|Independent Labour|Samuel George Hobson|2,506|19.5|-\\n[ROW] 4: Majority|Majority|Majority|1,463|11.3|-\\n[ROW] 5: Turnout|Turnout|Turnout|12,867|93.0|+5.9\\n[ROW] 6: -|Liberal gain from Conservative|Liberal gain from Conservative|Swing|-|-\\n\\nAnswer the following question based on the data above: \"Which candidate has more votes? Harvey or Royds?\". Generate SQL or Python code step-by-step given the question and table to answer the question correctly. For each step, generate SQL code to process the query or Python code to reformat the data. Output the code braced by \"```\" and an external executor will process the code generated and feed an intermediate table back to you. Answer the question directly if confident.\\n\\nSQL: ```SELECT candidate, votes FROM DF WHERE candidate LIKE \"%Harvey%\" OR candidate LIKE \"%Royds%\";```.\\n\\nIntermediate table:\\n[HEAD]: candidate|votes\\n---\\n[ROW] 1: Alexander Gordon Cummins Harvey|5,912\\n[ROW] 2: Clement Royds|4,449\\n\\nSQL: ```SELECT MAX(votes) - MIN(votes) FROM DF;```.\\n\\nIntermediate table:\\n[HEAD]: max(votes)_-_min(votes)\\n---\\n[ROW] 1: 1463.0\\n\\nAnswer: ```Harvey beat Royds by 1,463 votes.```.\\n\\nThe database table DF is shown as follows:\\n[HEAD]: year|nominee_work|award|result\\n---\\n[ROW] 1: 2004|Elephunk|Best Engineered Album, Non-Classical|Nominated\\n[ROW] 2: 2004|Where Is the Love? (ft. Justin Timberlake)|Best Rap/Sung Collaboration|Nominated\\n[ROW] 3: 2004|Where Is the Love? (ft. Justin Timberlake)|Record of the Year|Nominated\\n[ROW] 4: 2005|Let\\'s Get It Started|Record of the Year|Nominated\\n[ROW] 5: 2005|Let\\'s Get It Started|Best Rap Performance by a Duo or Group|Won\\n[ROW] 6: 2005|Let\\'s Get It Started|Best Rap Song|Nominated\\n[ROW] 7: 2005|Hey Mama|Best Rap Song|Nominated\\n[ROW] 8: 2006|Don\\'t Phunk with My Heart|Best Rap Song|Nominated\\n...\\n[ROW] 25: 2010|Boom Boom Pow|Best Dance Recording|Nominated\\n[ROW] 26: 2010|Boom Boom Pow|Best Short Form Music Video|Won\\n\\nAnswer the following question based on the data above: \"How did \"Don\\'t Phunk with My Heart\" from Will.i.am performed during its released in 2006?\". Generate SQL or Python code step-by-step given the question and table to answer the question correctly. For each step, generate SQL code to process the query or Python code to reformat the data. Output the code braced by \"```\" and an external executor will process the code generated and feed an intermediate table back to you. Answer the question directly if confident.\\n\\nSQL: ```SELECT * FROM DF WHERE year=2006 AND nominee_work=\"Don\\'t Phunk with My Heart\";```.\\n\\nIntermediate table:\\n[HEAD]: year|nominee_work|award|result\\n---\\n[ROW] 1: 2006|Don\\'t Phunk with My Heart|Best Rap Song|Nominated\\n[ROW] 2: 2006|Don\\'t Phunk with My Heart|Best Rap Performance by a Duo or Group|Won\\n\\nAnswer: ```In 2006, Will.i.am won the Grammy Award for Best Rap Performance by a Duo or Group for \"Don\\'t Phunk with My Heart\".```.\\n\\nThe database table DF is shown as follows:\\n[HEAD]: party|votes|percent|seats\\n---\\n[ROW] 1: Vietnamese Fatherland Front|10,561,314|100|424\\n[ROW] 2: Invalid/blank votes|105,807|–|–\\n[ROW] 3: Total|10,667,121|100|424\\n[ROW] 4: Source: IPU|Source: IPU|Source: IPU|Source: IPU\\n\\nAnswer the following question based on the data above: \"What were the voting results of the 1975 North Vietnamese legislative election with regards to seats?\". Generate SQL or Python code step-by-step given the question and table to answer the question correctly. For each step, generate SQL code to process the query or Python code to reformat the data. Output the code braced by \"```\" and an external executor will process the code generated and feed an intermediate table back to you. Answer the question directly if confident.\\n\\nSQL: ```SELECT seats FROM DF WHERE party=\"Vietnamese Fatherland Front\";```.\\n\\nIntermediate table:\\n[HEAD]: seats\\n---\\n[ROW] 1: 424',\n",
       " 'execution_match': None,\n",
       " 'gpt_error': None,\n",
       " 'execution_err': None,\n",
       " 'predicted_sql': None,\n",
       " 'df_reformat_sql': None,\n",
       " 'gpt_original_output': ['SQL: ```SELECT seats FROM DF WHERE party=\"Vietnamese Fatherland Front\";',\n",
       "  'Answer: ```The Vietnamese Fatherland Front won 424 seats.'],\n",
       " 'training_demo_ids': []}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(a['prompt'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e42c41ea-ed0b-4cdd-8dca-ec3a5762ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "46f05d0b-caad-4b4c-bc14-dbd3998b3a20",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'rouge_score' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrouge_score\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'rouge_score' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "rouge_score.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced8d2d-cd2e-4f39-8d10-d451e6b8c7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
